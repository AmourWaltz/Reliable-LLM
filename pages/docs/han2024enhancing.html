<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <meta name="viewport" content="width=device-width, minimal-ui, initial-scale=1, viewport-fit=cover">
        <title>Enhancing Confidence Expression in Large Language Models Through Learning from Past Experience</title>
        <link rel="icon" type="image/png" href="assets/images/favicon.png">
        <link href="assets/css/main.css" rel="stylesheet">
    </head>
    <body>
        <h1 id="enhancing-confidence-expression-in-large-language-models-through-learning-from-past-experience">Enhancing Confidence Expression in Large Language Models Through Learning from Past Experience</h1>
<p>[<a href="https://arxiv.org/abs/2404.10315">Link</a>]</p>
<h2 id="motivation">Motivation</h2>
<ul>
<li>Leveraging the intrinsic ability of LLMs or the signals from the output logits of answers proves challenging in accurately capturing the response uncertainty in LLMs.</li>
<li>When verbalizing their confidence, LLMs tend to exhibit high confidence.</li>
<li>It is challenging to obtain accurate confidence scores of LLMs due to the context sensitivity.</li>
</ul>
<h3 id="research-questions">Research Questions</h3>
<ol>
<li>How to capture the inherent confidence of the LLM? </li>
<li>How to teach the LLM to express confidence?</li>
<li>How to evaluate the confidence expression of the LLM?</li>
</ol>
<h2 id="methodology">Methodology</h2>
<h3 id="learning-from-past-experience-lepe">Learning from Past experience (LePe)</h3>
<ol>
<li><strong>Testing</strong>: The testing stage primarily aims to capture the inherent confidence of LLM by assessing its performance across a predefined set of questions. </li>
<li><strong>Learning</strong>: The LLM is fine-tuned using a curated set of instruction data to learn to express its confidence level. </li>
<li><strong>Predicting</strong>: The prediction stage involves the LLM applying its newly acquired ability to express confidence when addressing new, unseen questions.</li>
</ol>
<p><img src="../imgs/han2024enhancing/image.png" alt="alt text"></p>
<p>Design a complete pipeline and several strategies to alleviate context sensitivity from multiple aspects, including mutation questions and hybrid sampling strategies.</p>
<p><img src="../imgs/han2024enhancing/image-1.png" alt="alt text"></p>
<ul>
<li>The <strong>Mutation questions</strong> is to make various transformations of the questions and options without changing the original questions to test the robustness of the answers generated by LLM. </li>
<li>The <strong>Hybrid Sampling</strong> strategy uses multiple sampling methods to obtain the model’s intrinsic beliefs.</li>
</ul>
<h3 id="research-questions-1">Research Questions</h3>
<ol>
<li><em>How to capture the inherent confidence of the LLM?</em> Since different LLMs demonstrate varied proficiency levels within the same knowledge domain, it’s necessary to devise a standardized procedure to capture the inherent confidence of LLMs.</li>
<li><em>How to teach the LLM to elicit confidence?</em> After gathering confidence scores of the LLM, it becomes crucial to investigate effective strategies that enable the models to convey their confidence levels.</li>
</ol>
<p>(3) How to evaluate the confidence elicitation of the LLM? A comprehensive assessment of LLMs’ confidence elicitation abilities is required.</p>
<h2 id="experiments">Experiments</h2>
<h3 id="datasets">Datasets</h3>
<ul>
<li>Chinese: C-Eval and XieZhi.</li>
<li>English: GSM8K and CommonsenseQA.</li>
</ul>
<h3 id="models">Models</h3>
<ul>
<li>CuteGPT1 and LLaMA2-Chat2, ChatGPT.</li>
</ul>
<h3 id="metrics">Metrics</h3>
<ul>
<li>Accuracy.</li>
<li>ECE.</li>
<li>Pearson Correlation Coefficient (r).</li>
</ul>
<h3 id="findings">Findings</h3>
<ol>
<li>After using the LePe method, there is a strong correlation between the model’s prediction confidence level and the actual accuracy.</li>
<li>The acceptable confidence threshold serves as a crucial guideline in practical applications.</li>
<li>The calibration capability using LePe remains effective when tested with out-of-domain datasets.</li>
</ol>

    </body>
</html>