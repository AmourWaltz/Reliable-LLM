<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <meta name="viewport" content="width=device-width, minimal-ui, initial-scale=1, viewport-fit=cover">
        <title>Relying on the Unreliable: The Impact of Language Models’ Reluctance to Express Uncertainty</title>
        <link rel="icon" type="image/png" href="assets/images/favicon.png">
        <link href="assets/css/main.css" rel="stylesheet">
    </head>
    <body>
        <h1 id="relying-on-the-unreliable-the-impact-of-language-models-reluctance-to-express-uncertainty">Relying on the Unreliable: The Impact of Language Models’ Reluctance to Express Uncertainty</h1>
<p>[<a href="https://arxiv.org/abs/2401.06730">Link</a>]</p>
<h2 id="motivation">Motivation</h2>
<ul>
<li>How LMs incorporate confidence about their responses via natural language and how downstream users behave in response to LM-articulated uncertainties.</li>
<li>The pragmatic implications of speakers of epistemic markers being AI systems, combined with dangers of overreliance on AI could drastically change their interpretation compared to human-spoken ones.</li>
</ul>
<p><img src="../imgs/zhou2024relying/image.png" alt="alt text"></p>
<h2 id="findings">Findings</h2>
<ul>
<li>LMs are reluctant to share model uncertainties.</li>
<li>LMs can be explicitly prompt to use epistemic markers, but are more likely to generate expressions of certainty than uncertainty.</li>
<li>Users are heavily reliant on LM generated expressions of high confidence (e.g., &quot;I’m sure it’s...&quot;), but are also surprisingly reliant on plain statements (e.g., &quot;The answer is...&quot;).</li>
<li>Process of reinforcement learning with human feedback (RLHF) as a key contributing factor and uncover that human annotators are biased against expressions of uncertainty.</li>
</ul>
<h2 id="epistemic-markers-in-language-models">Epistemic Markers in Language Models</h2>
<ul>
<li>Weakeners — expressions of uncertainty.</li>
<li>Strengtheners — expressions of certainty.</li>
</ul>
<h2 id="how-do-lms-use-epistemic-markers">How do LMs use Epistemic Markers?</h2>
<p>LMs prefer to respond with answers free of epistemic markers and when LMs do use epistemic modifiers, they rely too much on strengtheners, leading to overconfident but incorrect generations.</p>
<h3 id="prompt-design">Prompt Design</h3>
<ul>
<li>Epistemic markers <em>“Please answer the question and provide your certainty level”</em>.</li>
<li>Chain-of-thought reasoning (CoT), <em>“Explain your thought process step by step”</em>.</li>
<li>A combination of both <em>“Using expressions of uncertainty, explain your thought process step by step”</em>.</li>
</ul>
<h3 id="models">Models</h3>
<ul>
<li>text-davinci-003, GPT-3.5-Turbo, GPT-4, LLaMA-2 7B, LLaMA-2 13B, LLaMA-2 70B, Claude-1, Claude-2, ClaudeInstant-1.</li>
</ul>
<h3 id="datasets">Datasets</h3>
<ul>
<li>MMLU.</li>
</ul>
<h3 id="findings-1">Findings</h3>
<ul>
<li>Models are reluctant to reveal uncertainties, but can be encouraged.</li>
<li>Models are biased towards using strengtheners.</li>
<li>Overconfidence results in confident but inaccurate generations.</li>
</ul>
<h3 id="eliciting-and-classifying-epistemic-markers">Eliciting and Classifying Epistemic Markers</h3>
<p>In contrast to previous analyses that primarily examined single-word lexical forms of epistemic markers like “anyway”, “should,” and “obviously”, our analysis specifically focuses on phraselevels of epistemic markers.</p>
<p><strong>Motivation</strong>: must examine how to mitigate the harms of model overconfidence and how to best build cognitive forcing designs</p>
<h2 id="human-interpretations-of-uncertainty">Human Interpretations of Uncertainty</h2>
<p>Users by default are <strong>highly reliant on LM-generated responses</strong> and that even minor miscalibrations in systems can have long-term consequences in human-LM collaborations.</p>
<h3 id="methods">Methods</h3>
<ul>
<li>Creating a Self-Incentivized Task.</li>
<li>Trivia Question Selection</li>
<li>Recruitment Process</li>
<li>Template Selection</li>
</ul>
<h3 id="settings">Settings</h3>
<ul>
<li>Setting 1: Control Setting: Users are asked whether or not they’d like to rely on Marvin’s answer, and since the users do not see any answers, they are simply expressing their reliance of epistemic markers as generated by LMs.</li>
</ul>
<p><img src="../imgs/zhou2024relying/image-1.png" alt="alt text"></p>
<ul>
<li>Setting 2: Interactive Settings: Providing users with feedback gives users the opportunity to build a mental model of how Marvin performs and allows us to measure the harms that may arise from long-term interaction.</li>
<li>Setting 2A: Calibrated Setting: Strengtheners appear with correct answers and weakeners appear with incorrect answers.</li>
<li>Setting 2B: Overconfident Setting.</li>
<li>Setting 2C: Underconfident Setting.</li>
</ul>
<h3 id="findings-2">Findings</h3>
<ul>
<li>Users rely on strengtheners but also on plain statements.</li>
<li>Users Effectively Leverage Calibrated LM-Generated Epistemic Markers: users are able to learn mental models of epistemic markers after approximately 20 rounds.</li>
<li>Users are Overreliant on Overconfident Responses.</li>
<li>Users in the Overconfident Setting Also Incorrectly Relied on Weakeners.</li>
<li>Users are Underreliant on Weakeners in Underconfident LMs</li>
</ul>
<h3 id="discussion">Discussion</h3>
<ul>
<li>Plain Statements are Confident Statements.</li>
<li>Miscalibrations in Strengtheners Impact Interpretation of Weakeners: Miscalibration leads users to distrust other areas of epistemic markers.</li>
<li>Long-Term Effects of Overconfidence: mental models of language models are developed early in LM-interactions, potentially resulting in long-term harms.</li>
</ul>
<h2 id="origin-of-model-overconfidence">Origin of Model Overconfidence</h2>
<h3 id="methods-1">Methods</h3>
<h4 id="model-stage">Model Stage</h4>
<p>Measure how base models and supervised fine-tuned models compare to their RLHF counterparts when it comes to generating expressions of certainty.</p>
<h4 id="reward-modeling">Reward Modeling</h4>
<p>Prompt the model with a question-response pair where the question is &quot;What is the capital of X?&quot; and the response is an epistemic marker like &quot;I think it’s&quot;.</p>
<h4 id="human-annotated-datasets">Human Annotated Datasets</h4>
<p>Measure how often strengtheners and weakeners are preferred by human annotators in these datasets.</p>
<h3 id="findings-3">Findings</h3>
<ul>
<li>Overconfidence in RLHF Models: This preference for strengtheners is introduced during the RLHF process.</li>
<li>Reward Modeling Is Biased Towards Certainty. </li>
<li>Human Raters are Biased Against Uncertainty.</li>
</ul>
<h3 id="discussion-1">Discussion</h3>
<h4 id="uncovering-unknowns-in-rlhf">Uncovering Unknowns in RLHF</h4>
<ul>
<li>Humans have implicit biases towards other dimensions of language which may not be known in the annotation phase.</li>
<li>The human bias against uncertain language is particularly harmful as it causes RLHF models to be reluctant in their generation uncertainty, negatively impacting human over-reliance on LMs.</li>
</ul>
<h4 id="beyond-mimicking-human-language">Beyond Mimicking Human Language</h4>
<ul>
<li>When it comes to expressions of uncertainty, mimicking humans (or mimicking what humans prefer) might not be the end goal.</li>
<li>Could design LMs to verbalize uncertainty in ways that would increase cognitive engagement and lower human overreliance on imperfect models.</li>
</ul>
<h2 id="my-comments">My Comments</h2>
<blockquote>
<p>Although I think this paper is more suitable for Human-Computer Interaction field, it actually provide many insights for future work of LLM uncertainty research.</p>
</blockquote>

    </body>
</html>