<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <meta name="viewport" content="width=device-width, minimal-ui, initial-scale=1, viewport-fit=cover">
        <title>Uncertainty in Language Models: Assessment through Rank-Calibration</title>
        <link rel="icon" type="image/png" href="assets/images/favicon.png">
        <link href="assets/css/main.css" rel="stylesheet">
    </head>
    <body>
        <h1 id="uncertainty-in-language-models-assessment-through-rank-calibration">Uncertainty in Language Models: Assessment through Rank-Calibration</h1>
<p>[<a href="https://arxiv.org/abs/2404.03163">Link</a>]</p>
<h2 id="motivation">Motivation</h2>
<ul>
<li>It is crucial to correctly quantify their uncertainty in responding to given inputs. </li>
<li>Many uncertainty measures (e.g., verbalized confidence elicited via prompting, semantic entropy and affinitygraph-based measures) can differ greatly, and <strong>it is unclear how to compare them, partly because they take values over different ranges</strong>.</li>
<li>Uncertainty measures are more general and arguably more principled than confidence measures for LMs, but they <strong>lack a universal assessment metric</strong> such as ECE. A key issue is that uncertainty measures are not necessarily commensurate.</li>
</ul>
<h3 id="difficulties-of-confidence-measure-for-nlg-tasks">Difficulties of Confidence Measure for NLG Tasks</h3>
<ol>
<li>The label space of produced textual responses is often exponentially too large to assess correctness with respect to the output length for any given input.</li>
<li>Logits encode the likelihood of selecting the next token and do not necessarily capture linguistic sense.</li>
<li>Even hand-crafted prompts intended to make LMs express confidence explicitly may not lead to reliable confidence values because elicitation is heavily tied to prompt formats.</li>
</ol>
<h2 id="limitations-of-existing-assessments">Limitations of Existing Assessments</h2>
<h3 id="ad-hoc-correctness-thresholding">Ad hoc correctness thresholding</h3>
<p>The relative AUROC results of distinct measures vary drastically with the choice of τ. 
This is especially concerning given that there seems to be no principled way to set this threshold.</p>
<h3 id="diverse-output-ranges">Diverse output ranges</h3>
<p>The second limitation of existing assessments is rooted in the diverse output ranges of the uncertainty or confidence measures.</p>
<h3 id="strong-dependence-on-lm-performance">Strong dependence on LM performance</h3>
<p>This is undesirable because our goal is to provide an overall assessment of the uncertainty measure, which may in the future need to be applied to different LMs.</p>
<h3 id="desiderata-of-evaluation">Desiderata of evaluation</h3>
<h2 id="methodology">Methodology</h2>
<h3 id="workflow">Workflow</h3>
<p><img src="../imgs/huang2024uncertainty/image.png" alt="alt text"></p>
<h3 id="rank-calibration">Rank-Calibration</h3>
<ol>
<li>An uncertainty measure U is rank-calibrated if (1) holds for any u in U ’s range: on average, lower uncertainty implies higher generative quality.</li>
<li>Higher values of a confidence measure should imply higher generation accuracy.</li>
</ol>
<h3 id="advantages">Advantages</h3>
<ol>
<li>The empirical RCE does not require any thresholding of the correctness values. </li>
<li>Rankcalibration assesses the monotonicity of uncertainty values by leveraging relative ranks, which makes it independent of the output range. </li>
<li>Similar to ECE, the RCE is not directly tied to the generation performance of the LM. </li>
<li>The assessment is practical for any uncertainty/confidence measures.</li>
</ol>
<h2 id="experiments">Experiments</h2>
<h3 id="datasets">Datasets</h3>
<ul>
<li>TriviaQA, Natural Questions, SQuAD1, and Meadow.</li>
</ul>
<h3 id="models">Models</h3>
<ul>
<li>Llama-2-7b, Llama-2-7b-chat, GPT-3.5-turbo.</li>
</ul>
<h3 id="metrics">Metrics</h3>
<ul>
<li>Rouge-L score, BERT similarity, and ChatGPT evaluation.</li>
</ul>
<h2 id="contributions">Contributions</h2>
<ul>
<li>Mathematically formalize the assessment of uncertainty/confidence measures for LMs in NLG tasks.<blockquote>
<p>I think the mathematical definitions are good!</p>
</blockquote>
</li>
<li>Demonstrate existing assessment metrics (e.g., AUROC, ECE, etc) have several limitations, including a heavy dependence on the LM’s performance, instability caused by ad hoc binarization of correctness scores, and incompatibility with diverse uncertainty ranges.</li>
<li>Lower uncertainty/higher confidence should indicate higher-quality generation. Propose assessing uncertainty measures in terms of rank-calibration and introduce a suitable metric, the Rank-Calibration Error (RCE).</li>
<li>Introduce the Empirical RCE—an estimate of RCE based on a finite dataset to make rank-calibration practical.</li>
<li>Demonstrate the broader applicability and granular interpretability of the proposed methods.</li>
</ul>

    </body>
</html>