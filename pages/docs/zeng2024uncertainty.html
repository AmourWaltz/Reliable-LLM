<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <meta name="viewport" content="width=device-width, minimal-ui, initial-scale=1, viewport-fit=cover">
        <title>Uncertainty is Fragile: Manipulating Uncertainty in Large Language Models</title>
        <link rel="icon" type="image/png" href="assets/images/favicon.png">
        <link href="assets/css/main.css" rel="stylesheet">
    </head>
    <body>
        <h1 id="uncertainty-is-fragile-manipulating-uncertainty-in-large-language-models">Uncertainty is Fragile: Manipulating Uncertainty in Large Language Models</h1>
<p>[<a href="https://www.arxiv.org/abs/2407.11282">Link</a>]</p>
<h2 id="motivation">Motivation</h2>
<ul>
<li>It is crucial to correctly quantify their uncertainty in responding to given inputs. </li>
<li>Many uncertainty measures (e.g., verbalized confidence elicited via prompting, semantic entropy and affinitygraph-based measures) can differ greatly, and <strong>it is unclear how to compare them, partly because they take values over different ranges</strong>.</li>
<li>Uncertainty measures are more general and arguably more principled than confidence measures for LMs, but they <strong>lack a universal assessment metric</strong> such as ECE. A key issue is that uncertainty measures are not necessarily commensurate.</li>
</ul>
<h2 id="methodology">Methodology</h2>
<p><img src="../imgs/zeng2024uncertainty/image.png" alt="alt text"></p>
<ul>
<li>Apply KL divergence to adjust the modelâ€™s uncertainty to approximate a uniform distribution in the presence of backdoor tokens, while maintaining the original answer distribution unchanged when no backdoors are present.</li>
</ul>
<h3 id="threat-model">Threat Model</h3>
<ul>
<li>Attacker Objective - for LLMs that are regarded as well-calibrated to become completely miscalibrated when the prompts are embedded with backdoor triggers.</li>
<li>Attacker Capabilities - This attacker can also access some datasets from the internet and make them into poisoned datasets <strong>without knowledge of the pre-training details</strong>.</li>
</ul>
<h3 id="backdoor-triggers">Backdoor Triggers</h3>
<ul>
<li>Text backdoor trigger - inserting one short human-curated string into the input prompt.</li>
<li>Syntactic trigger - not lead to dramatic semantic changes in the prompt compared to the simple text trigger.</li>
<li>Style backdoor triggers - using GPT-4 to reformulate the prompt before questions into Shakespearean style.</li>
</ul>
<p><img src="../imgs/zeng2024uncertainty/image-1.png" alt="alt text"></p>
<h3 id="backdoor-injection">Backdoor Injection</h3>
<p><img src="../imgs/zeng2024uncertainty/image-2.png" alt="alt text"></p>
<p>A bi-level optimization problem, optimizing both the original prompt tuning task and the backdoor task.</p>
<p><img src="../imgs/zeng2024uncertainty/image-3.png" alt="alt text">
<img src="../imgs/zeng2024uncertainty/image-4.png" alt="alt text">
<img src="../imgs/zeng2024uncertainty/image-5.png" alt="alt text"></p>
<p>Fine-tuning LLMs by minimizing the KL divergence between target attacked distribution and original distribution while maintaining the correct answewr.</p>
<h2 id="experiments">Experiments</h2>
<h3 id="datasets">Datasets</h3>
<ul>
<li>MMLU, CosmosQA, HellaSwag, HaluDial, HaluSum, CNN/Daily Mail.</li>
</ul>
<h3 id="models">Models</h3>
<ul>
<li>QWen2-7B, LLaMa3-8B, Mistral-7B and Yi-34B using LoRA.</li>
</ul>
<h3 id="metrics">Metrics</h3>
<ul>
<li>Uncertainty Metrics: entropy uncertainty and conformal prediction.</li>
<li>Benign Accuracy: the accuracy of the attacked model should be close to the original model.</li>
<li>Attack Success Rate (ASR): the success of attacks as the rate at which the uncertainty of test instances with a backdoor exceeds their uncertainty without a backdoor.</li>
</ul>
<h2 id="analysis">Analysis</h2>
<ul>
<li><em>How many samples are enough for attacking the uncertainty?</em> The attack success rate tends to stabilize, converging around the 1000-step mark. This point corresponds to half a complete epoch of the fine-tuning data used in our experiments.</li>
<li><em>Do attacks transfer across different prompts?</em> Despite the change in prompt, our attack still managed to achieve considerable success rates in most models.</li>
<li><em>Do attacks generalize across different domains of texts?</em> The uncertainty attack exhibits a significant ability to generalize across different domains, which underscores its potential impact.</li>
</ul>

    </body>
</html>