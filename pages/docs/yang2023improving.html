<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <meta name="viewport" content="width=device-width, minimal-ui, initial-scale=1, viewport-fit=cover">
        <title>Improving the Reliability of Large Language Models by Leveraging Uncertainty-Aware In-Context Learning</title>
        <link rel="icon" type="image/png" href="assets/images/favicon.png">
        <link href="assets/css/main.css" rel="stylesheet">
    </head>
    <body>
        <h1 id="improving-the-reliability-of-large-language-models-by-leveraging-uncertainty-aware-in-context-learning">Improving the Reliability of Large Language Models by Leveraging Uncertainty-Aware In-Context Learning</h1>
<p>[<a href="https://arxiv.org/abs/2310.04782">Link</a>]</p>
<h2 id="motivation">Motivation</h2>
<ul>
<li>&quot;Uncertainty is lower when the model’s response is correct compared to when it is incorrect.” </li>
<li>Setting a strict uncertainty threshold to discern the correctness of the model’s response seems intuitive, but it presents a formidable challenge in practice.</li>
</ul>
<h2 id="method">Method</h2>
<p><img src="../imgs/yang2023improving/image.png" alt="alt text"></p>
<ol>
<li>For each question in the calibration dataset, the model generates multiple responses, each labeled as “correct” or “incorrect”, with corresponding uncertainty calculations. </li>
<li>When all of the model’s responses for a particular question are classified as “incorrect”, the uncertainty-aware model should refrain from providing an answer. </li>
<li>Conversely, if at least one of the model’s responses aligns with the correct answer, the uncertainty-aware model should select the correct response as its final answer.</li>
</ol>
<h3 id="overview">Overview</h3>
<p><img src="../imgs/yang2023improving/image-1.png" alt="alt text"></p>
<ul>
<li>The uncertainty-aware framework is divided into two steps, namely <strong>uncertainty score calculation</strong> and <strong>uncertainty-aware correction</strong>.<ol>
<li>Utilize the logit output values of the model’s response to obtain the uncertainty of each generated token.</li>
<li>Aggregate these token-level uncertainties to derive the uncertainty of each generated output.</li>
</ol>
</li>
<li>To explore the knowledge boundary of the model and investigate the beam search space within the answer set, make the model responds to the same question multiple times.</li>
<li>After obtaining the uncertainty score for each response, we supervised fine-tuning the model for self-correction. This fine-tuning process empowers the model to autonomously adjust its responses based on the calculated uncertainty scores.</li>
</ul>
<h4 id="uncertainty-score-calculation">Uncertainty Score Calculation</h4>
<p><strong>Uncertainty Score Estimation Method</strong></p>
<ul>
<li>Single-inference Based Uncertainty Estimation.<ul>
<li>Minimum of Token Probabilities.</li>
<li>Average of Token Probabilities.</li>
<li>Normalized Product of Token Probabilities.</li>
<li>Log-sum of Token Probabilities.</li>
</ul>
</li>
<li>Multi-inference Based Uncertainty Estimation. (Not adapt)</li>
</ul>
<p><strong>Uncertainty Score Mapping</strong></p>
<ul>
<li>Due to the highly <strong>uneven</strong> distribution of uncertainty in the training dataset, the direct inclusion of uncertainty introduces a great deal of noise and confusion.</li>
<li>In the experiment, we investigate the impact of various classification granularities, namely deciles, hundreds, and thousands, which correspond to uncertainty scores ranging from 1 to 10, 1 to 100, and 1 to 1000, respectively.</li>
<li>This same mapping strategy is applied during the inference stage, with uncertainty scores computed based on the intervals established during the training phase.</li>
</ul>
<h4 id="uncertainty-aware-correction">Uncertainty-Aware Correction</h4>
<p><strong>Uncertainty-Aware Fine-tuning</strong></p>
<ul>
<li>Rather than directly injecting correct answers from the calibration dataset into the model, we employ it to assess the correctness of the model’s responses.</li>
<li>In cases where all of the model’s responses for a particular question are categorized as “incorrect,” we interpret this as an indication that the model lacks the requisite knowledge to respond to the question</li>
<li>When at least one of the model’s responses aligns with the correct answer, we infer that the model possesses the requisite knowledge to address the question.</li>
</ul>
<p><strong>Test-time Correction</strong></p>
<ul>
<li>The model respond to each question once and calculate its corresponding uncertainty score. </li>
<li>Then resend the problem, the response and the uncertainty score to the model for <strong>self-correction</strong>.</li>
</ul>
<h2 id="experiment">Experiment</h2>
<h3 id="tow-proposed-metrics">Tow Proposed Metrics</h3>
<ul>
<li>Accuracy (“refused questions” and “answered questions.”)</li>
<li>AUROC.</li>
</ul>
<h3 id="datasets">Datasets</h3>
<ul>
<li>SciQ.</li>
</ul>
<h3 id="models">Models</h3>
<ul>
<li>Vicuna and LLaMA.</li>
</ul>
<h3 id="findings">Findings</h3>
<ul>
<li>Analysis of different uncertainty estimation methods - Log-sum is optimal.</li>
<li>Analysis of mean uncertainty score and AUROC - Underscores the presence of a correlation between the mean uncertainty of responses and overall accuracy.</li>
</ul>
<p><img src="../imgs/yang2023improving/image-2.png" alt="alt text"></p>
<h3 id="case-study">Case Study</h3>
<ol>
<li>The uncertainty scores associated with correct responses do not consistently fall below those of incorrect responses.</li>
<li>During testing, responses with high uncertainty scores may be rejected, even if the model has the necessary knowledge to generate an answer to <strong>improve reliability</strong>.</li>
<li>The model consistently modifies its behavior to reject responses when it encounters questions that fall beyond its knowledge domain.</li>
</ol>
<h2 id="discussion">Discussion</h2>
<ul>
<li>Is there any better uncertainty estimation methods?</li>
<li><strong>How to deal with over-confidence problem?</strong>: The model’s resilience to variations in question content may serve as a potential indicator to judge whether a given response is a over-confidence question or not.</li>
<li>Data uncertainty and model uncertainty: Unknown questions contribute to data uncertainty.</li>
</ul>

    </body>
</html>