<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <meta name="viewport" content="width=device-width, minimal-ui, initial-scale=1, viewport-fit=cover">
        <title>Preserving Pre-trained Features Helps Calibrate Fine-tuned Language Models</title>
        <link rel="icon" type="image/png" href="assets/images/favicon.png">
        <link href="assets/css/main.css" rel="stylesheet">
    </head>
    <body>
        <h1 id="preserving-pre-trained-features-helps-calibrate-fine-tuned-language-models">Preserving Pre-trained Features Helps Calibrate Fine-tuned Language Models</h1>
<p>[<a href="https://arxiv.org/abs/2305.19249">Link</a>]</p>
<h2 id="motivation">Motivation</h2>
<ul>
<li>Fine-tuned models still suffer from <strong>overconfident predictions</strong>, especially in out-of-domain settings.</li>
<li>The PLMs are well-calibrated on the masked language modeling task with robust predictive confidence under domain shift, yet the fine-tuned models fail to retain such property due to <strong>catastrophic forgetting</strong>, which impacts the calibration on the downstream classification task.</li>
</ul>
<h2 id="methodology">Methodology</h2>
<h3 id="a-closed-look">A Closed Look</h3>
<ul>
<li>The PLM is relatively well-calibrated across different domains.</li>
<li>Although the hidden representations of 50% masked inputs have shifted significantly from the original input, the PLM can still make calibrated predictions to the original inputs.</li>
<li>Fine-tuning changes the hidden representation of the LM in two ways: (1) For the inputs within the same domain, fine-tuning enlarges the difference of the corresponding hidden representations. (2) For the inputs across different domains, fine-tuning makes the hidden representations from different domains much harder to distinguish by projecting them to a simpler data manifold, which causes the fine-tuned model fails to give proper predictive confidence for OD and outlier samples.</li>
</ul>
<h3 id="baselines-pretrained-model-feature-preserving-to-address-catastrophic-forgetting">Baselines (Pretrained Model Feature Preserving to Address Catastrophic Forgetting)</h3>
<ul>
<li>Parameter-efficient tuning.</li>
<li>Pre-trained weight decay.</li>
<li>Mixout (stochastically replaces the model parameters with their pre-trained counterparts with probability p at each training iteration).</li>
</ul>
<h3 id="joint-learning-with-mlm-objective">Joint Learning with MLM Objective</h3>
<ul>
<li>Jointly optimize the MLM objective in the fine-tuning phase.</li>
<li>Three simple techniques to further strengthen the connection between the pre-trained model and our fine-tuned model.<ul>
<li>Utilizing corpus of the pre-training phase.</li>
<li>Distillation from the pre-trained model.</li>
<li>Regularization on the contextualized representation.</li>
</ul>
</li>
</ul>
<h2 id="experiments">Experiments</h2>
<h3 id="datasets">Datasets</h3>
<ul>
<li>Natural Language Inference (NLI): SNLI (in-domain) and MNLI (out-of-domain).</li>
<li>Paraphrase Detection (PD): QQP (in-domain) and TwitterPPDB (out-of-domain).</li>
<li>Commonsense Reasoning: SWAG (in-domain) and HellaSWAG (out-of-domain).</li>
</ul>
<h3 id="models">Models</h3>
<ul>
<li>RoBERTa_{BASE}.</li>
</ul>
<h3 id="metrics">Metrics</h3>
<ul>
<li>Accuracy.</li>
<li>ECE.</li>
</ul>

    </body>
</html>