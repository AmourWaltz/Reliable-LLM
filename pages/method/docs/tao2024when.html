<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <meta name="viewport" content="width=device-width, minimal-ui, initial-scale=1, viewport-fit=cover">
        <title>When to Trust LLMs: Aligning Confidence with Response Quality and Exploring Applications in RAG</title>
        <link rel="icon" type="image/png" href="assets/images/favicon.png">
        <link href="assets/css/main.css" rel="stylesheet">
    </head>
    <body>
        <h1 id="when-to-trust-llms-aligning-confidence-with-response-quality-and-exploring-applications-in-rag">When to Trust LLMs: Aligning Confidence with Response Quality and Exploring Applications in RAG</h1>
<p>[<a href="https://arxiv.org/abs/2404.17287">Link</a>]</p>
<h2 id="motivation">Motivation</h2>
<ul>
<li>The urgent question is: When can we trust LLMs?</li>
<li>Existing methods, which rely on verbalizing confidence to tell the reliability by inducing top-k responses and sampling-aggregating multiple responses, often fail, due to the <strong>lack of objective guidance of confidence</strong>.</li>
</ul>
<h2 id="methodology">Methodology</h2>
<p>Assign high scores to well-aligned response-confidence pairs while assigning low scores to misaligned ones.</p>
<p><img src="../imgs/tao2024when/image.png" alt="alt text"></p>
<h3 id="confidence-alignment-via-rl">Confidence Alignment via RL</h3>
<ol>
<li>The construction of training data begins with the generation of a dataset containing tuples in the format: *&lt;question, response, confidence score&gt;*.</li>
<li>This alignment gain comes at the cost of reduced response accuracy, which can be attributed to the distinct objectives used for response quality and confidence alignment in the RL fine-tuning process.</li>
<li>Therefore, aim to align confidence with response quality without degrading the quality itself.</li>
</ol>
<h3 id="confidence-quality-order-preserving-alignment-conqord">CONfidence-Quality-ORDer-preserving Alignment (CONQORD)</h3>
<h4 id="dual-component-reward-strategy">Dual-component Reward Strategy</h4>
<ol>
<li>For response quality, train a <strong>quality reward model</strong> that rates the response.</li>
<li>For confidence alignment, we design an <strong>order-preserving criterion</strong> that maintains a consistent ordinal relationship between the verbalized confidence and response quality rating.</li>
<li>With this dual-component reward function, comprising both the quality reward model and the alignment reward function, employ <strong>Proximal Policy Optimization (PPO)</strong> algorithm to align the verbalized confidence with the quality of the response, without causing the model to become overcautious.</li>
</ol>
<ul>
<li><strong>Quality reward:</strong> Utilize Reinforcement Learning from Human Feedback (RLHF) datasets, ensuring that the high-quality response receive a higher score than the low-quality ones.</li>
<li><strong>Order-preserving alignment criterion:</strong> This criterion is grounded in the intuition that a higher quality response should be accompanied by a higher stated confidence.</li>
<li><strong>Order-preserving alignment reward:</strong> The reward function is defined as the sum of the products of pairwise differences in confidence and corresponding reward scores for all samples.</li>
</ul>
<h3 id="rl-fine-tuning-llm">RL Fine-tuning LLM</h3>
<p>Employing the dual-component reward as an approximation of the golden reward and the vanilla pre-trained LLM as the policy π for optimization.</p>
<h3 id="comparison-with-preapproach">Comparison with PreApproach</h3>
<ul>
<li>PreApproach manually assigns confidence scores to construct samples for fine-tuning reward model data, which is susceptible to introducing bias.</li>
<li>CONQORD method introduces an order-preserving alignment reward function that circumvents this issue by not requiring explicit confidence specification.</li>
</ul>
<h2 id="experiments">Experiments</h2>
<h3 id="datasets">Datasets</h3>
<ul>
<li>TruthfulQA, Natural Questions (NQ).</li>
</ul>
<h3 id="baselines">Baselines</h3>
<ul>
<li>Vanilla, Top-k, CoT+Agg.</li>
</ul>
<h3 id="models">Models</h3>
<ul>
<li>LLAMA 2 7B and 13 B.</li>
</ul>
<h3 id="metrics">Metrics</h3>
<ul>
<li>Expected Calibration Error (ECE)</li>
<li>Pearson Correlation Coefficient (PCC) - the linear relationship between two data sets</li>
<li>Spearman’s Rank Correlation Coefficient (SRCC) - the rank-based correlation between two variables.</li>
<li>Accuracy - GPT-4 to evaluate.</li>
</ul>
<h3 id="findings">Findings</h3>
<ol>
<li>CONQORD maintains the base model’s performance while significantly improving calibration, unlike PreApproach, which causes a notable performance drop.</li>
<li>When compared to the CoT prompt that enhances performance, CONQORD still has room for improvement.</li>
<li>Introducing retrieval augmentation for high-confidence responses may lead to unexpected performance degradation.</li>
<li>Choosing an appropriate confidence threshold in practical applications enables us to fully leverage the model’s inference capability while minimizing unnecessary retrieval and avoiding noisy information.</li>
</ol>

    </body>
</html>