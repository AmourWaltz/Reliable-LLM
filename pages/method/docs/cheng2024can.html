<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <meta name="viewport" content="width=device-width, minimal-ui, initial-scale=1, viewport-fit=cover">
        <title>When to Trust LLMs: Aligning Confidence with Response Quality and Exploring Applications in RAG</title>
        <link rel="icon" type="image/png" href="assets/images/favicon.png">
        <link href="assets/css/main.css" rel="stylesheet">
    </head>
    <body>
        <h1 id="when-to-trust-llms-aligning-confidence-with-response-quality-and-exploring-applications-in-rag">When to Trust LLMs: Aligning Confidence with Response Quality and Exploring Applications in RAG</h1>
<p>[<a href="https://arxiv.org/abs/2401.13275">Link</a>]</p>
<h2 id="motivation">Motivation</h2>
<ul>
<li>Although LLMs possess intensive world knowledge, they still make factual errors when facing some knowledge intensive tasks.</li>
<li>When the assistant’s output contains factual errors, it indicates that it may lack the corresponding knowledge internally, yet it fails to express the unknowns and refuse to give a answer.</li>
</ul>
<h2 id="outline">Outline</h2>
<p><strong>Knowledge Quadrants</strong></p>
<ul>
<li><strong>Known Knowns</strong> is crucial for a truthful AI assistant, as it relies on its own knowledge to provide accurate and reliable responses.</li>
<li><strong>Known Unknowns</strong> - doesn’t have information on a topic or when the information is not certain to maintain truthfulness.</li>
<li><strong>Unknown Unknowns</strong> and <strong>Unknown Knowns</strong> will cause untruthful and helpless generations.</li>
</ul>
<p><strong>Goal</strong>: Teach AI assistants to know what they know and what they do not know to convert Unknown Knowns and Unknown Unknowns to Known Knowns and Known Unknowns.</p>
<p><strong>Approach</strong>: Align an AI assistant with a model-specific “I don’t know” (Idk) dataset which contains the assistant’s known and unknown questions.</p>
<ul>
<li>Idk-Prompting: Effective to some extent, but there are still numerous IDK-IK and IDK-IDK questions</li>
<li>Supervised fine-tuning (SFT): Refuse to answer some questions it knows, leading to a decrease in the number of IK-IK questions.</li>
<li>Preferenceaware optimization - best-of-n sampling (BoN), proximal policy optimization (PPO), direct preference optimization (DPO) and hindsight instruction relabeling (HIR): Mitigate the phenomenon where the model incorrectly refuses to answer questions it knows.</li>
</ul>
<p><img src="../imgs/cheng2024can/image.png" alt="alt text"></p>
<h2 id="methodology">Methodology</h2>
<h3 id="construction-of-the-idk-dataset">Construction of the Idk Dataset</h3>
<p><img src="../imgs/cheng2024can/image-1.png" alt="alt text"></p>
<ol>
<li>Sample multiple responses from the model for each question, calculating the accuracy rate across these responses as a measure of the model’s confidence regarding that question.</li>
<li>Select a specific level of confidence as the criterion for determining whether the model knows or does not know the answer to a question, that is the Ik threshold.</li>
</ol>
<p><strong>Determine whether the output of a model is correct</strong> - lexical matching.
<strong>Meaning of different Ik thresholds</strong> - A high Ik threshold indicates that the model will only answer a question if it possesses a high level of confidence.</p>
<h3 id="idk-prompting">Idk Prompting</h3>
<blockquote>
<p>Answer the following question, and if you don’t know the answer, only reply with &quot;I don’t know&quot;: <Question></p>
</blockquote>
<h3 id="idk-supevised-fine-tuning">Idk Supevised Fine-tuning</h3>
<p>Directly use the Idk dataset for Supervised Fine-tuning of the model.</p>
<h3 id="preference-aware-optimization">Preference-aware Optimization</h3>
<p><strong>Direct Preference Optimization (DPO)</strong></p>
<ul>
<li>First train a SFT model on half of the Idk dataset as a warm up, then collect responses from this SFT model on the other half of the Idk data.</li>
<li>A preference data sample consists of a question, a chosen response, and a rejected response.</li>
<li>For questions the model knows, we use the correct response generated by it as the chosen response and “I don’t know” as the rejected response. For questions the model does not know, we use “I don’t know” as the chosen response and its incorrectly generated response as the rejected response.</li>
</ul>
<p><strong>Best-of-n Sampling (BoN)</strong></p>
<ul>
<li>Determine if the model knows the answer to a certain question by training a reward model to score the candidate responses.</li>
<li>First train a SFT model using a half of the Idk data and then use the SFT model to initialize the reward model.</li>
<li>Train the reward model using a pairwise loss.</li>
</ul>
<p><strong>Proximal Policy Optimization (PPO)</strong></p>
<p><strong>Hindsight Instruction Relabeling (HIR)</strong></p>
<blockquote>
<p>Your current knowledge expression confidence level is <X>, please answer the user’s question: <Question></p>
</blockquote>
<ul>
<li>Can control the model to adopt either a conservative or aggressive response strategy through the instruction, without the need to retrain the model.</li>
</ul>
<h2 id="experiments">Experiments</h2>
<h3 id="datasets">Datasets</h3>
<ul>
<li>TriviaQA, Natural Questions (NQ), ALCUNA.</li>
</ul>
<h3 id="models">Models</h3>
<ul>
<li>Llama-2-7b-chat.</li>
</ul>
<h3 id="metrics">Metrics</h3>
<ul>
<li>IK-IK Rate, IK-IDK Rate, TRUTHFUL Rate.</li>
</ul>
<h3 id="results">Results</h3>
<p><img src="../imgs/cheng2024can/image-2.png" alt="alt text"></p>
<ul>
<li><strong>Evaluation on out-of-distribution data</strong>: Model’s behavior of refusing to answer unknown questions can be generalized to OOD data.</li>
</ul>
<h3 id="ablation-study">Ablation Study</h3>
<ul>
<li><strong>Effect of model size</strong>: Larger models are more adpet at distinguishing between questions they know and do not know.</li>
<li><strong>Effect of data sources</strong>: Constructing a model-specific Idk dataset is necessary for enabling the model to learn to refuse to answer questions it does not know.</li>
<li><strong>Effect of IK threshold</strong>: Setting a high Ik threshold aids the model in better distinguish between knowledge it knows and does not know, making the model more truthful. The proportion of Idk questions in the dataset increases, the model tends to refuse to answer questions more frequently.</li>
</ul>
<h2 id="findings">Findings</h2>
<ul>
<li>After aligning using Idk datasets, AI assistants are capable of largely knowing what they know and what they do not know and refusing their unknown questions.</li>
<li>Supervised fine-tuning cause the model to become overly conservative; Preference-aware optimization can mitigate this problem.</li>
<li>The more questions labeled as ”I don’t know,” the more likely the assistant is to refuse to answer questions. In general, the higher the Ik threshold, the greater the total number of Ik-Ik and Ik-Idk questions, resulting in a more truthful assistant.</li>
<li>Larger model is more adept at distinguishing which questions it knows and which it doesn’t know.</li>
</ul>

    </body>
</html>