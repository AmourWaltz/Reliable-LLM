<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <meta name="viewport" content="width=device-width, minimal-ui, initial-scale=1, viewport-fit=cover">
        <title>Linguistic Calibration of Language Models</title>
        <link rel="icon" type="image/png" href="assets/images/favicon.png">
        <link href="assets/css/main.css" rel="stylesheet">
    </head>
    <body>
        <h1 id="linguistic-calibration-of-language-models">Linguistic Calibration of Language Models</h1>
<p>[<a href="https://arxiv.org/abs/2404.00474">Link</a>]</p>
<h2 id="motivation">Motivation</h2>
<ul>
<li>Existing models cannot produce text with calibrated confidence statements.</li>
<li>Currently, when an LM lacks knowledge about a topic, it will do one of two things: hallucinate incorrect claims with complete confidence, or, in the case of a few strong closed-source models, abstain from making claims.</li>
<li>Linguistic calibration — conveying confidence levels in natural language that equal the likelihood that one’s claims are correct—could mitigate the harms of hallucination.</li>
<li>The long-form, multi-claim generations that users encounter in practice have neither a single closed-form confidence nor a correctness; each generation contains information that answers many possible downstream questions.</li>
</ul>
<p><img src="../imgs/band2024linguistic/image-2.png" alt="alt text"></p>
<h2 id="preliminaries">Preliminaries</h2>
<ul>
<li><strong>Goal</strong>: Formulate a tractable objective that enables the end-to-end linguistic calibration of long-form LM generations.</li>
</ul>
<h3 id="user-decision-problem">User Decision Problem</h3>
<p><strong>Decision-making with LM assistance</strong>: The user first prompts an LM π with an open-ended query q. The LM generates a long-form context z.
Any decision task has an associated question x with answer y.
The true distribution over answers p(y | x) is unknown to the user.</p>
<h3 id="linguistic-calibration-by-calibrating-generations-to-forecasts">Linguistic Calibration by Calibrating Generations to Forecasts</h3>
<ul>
<li><strong>Problem</strong>: It is difficult to obtain real-world rewards from decision-making, and moreover to obtain a real-world distribution over queries to LMs and related user decision tasks.</li>
<li><em>Solution*</em>:</li>
<li>Optimizing user’s probabilistic forecast over answers will encourage the LM to generate contexts z that enable users to provide calibrated answers to decision task questions x.</li>
<li>Use knowledge-intensive question-answering datasets to generate a set of user decision tasks.</li>
</ul>
<p><strong>Linguistic calibration of long-form generations</strong> is an optimization procedure that calibrates an LM’s long-form generations in a way that leads to calibrated user forecasts.</p>
<h3 id="calibration-and-decision-making">Calibration and Decision-making</h3>
<ul>
<li>Calibration implies that Bayes-optimal decisions are zero expected regret.</li>
<li>Scoring rules measure the quality of a forecast. If a scoring rule is proper, the forecaster’s reward has the desirable property that it is maximized when the forecaster predicts the true probability.</li>
</ul>
<p><img src="../imgs/band2024linguistic/image.png" alt="alt text"></p>
<h3 id="linguistic-calibration-objective">Linguistic Calibration Objective</h3>
<p><img src="../imgs/band2024linguistic/image-1.png" alt="alt text"></p>
<p><strong>Guarantees for weaker notions of calibration</strong></p>
<h2 id="methodology">Methodology</h2>
<p><strong>Two-step Framework</strong></p>
<ul>
<li>Obtain an LM with some ability to express confidences in a long-form generation.</li>
<li>Use it as an RL policy and optimize our proper scoring rule objective end-to-end, with supervision from the surrogate task distribution.</li>
</ul>
<p><img src="../imgs/band2024linguistic/image-3.png" alt="alt text"></p>
<h3 id="generating-supervision-for-long-form-calibration">Generating Supervision for Long-Form Calibration</h3>
<ol>
<li>Sample a questionanswer pair (x, y) ∼ p(x, y) from a question-answering dataset.</li>
<li>LM query q such that z ∼ π(z | q) is a long-form generation salient to (x, y)</li>
</ol>
<h3 id="summary-distillation">Summary Distillation</h3>
<p>Summary distillation bootstraps a base LM πBase to have some ability to express its confidence in long-form natural language generations.
Follow a simple approach inspired by Self-Consistency, which obtains calibrated LM confidences for short answer questions by computing a statistic of many output samples.</p>
<ul>
<li>To obtain statements of confidence that are faithful to the base model’s internal confidence levels, prompt an API-based LLM to summarize these samples into a single consensus paragraph s with statements of confidence based on the frequency of claims.</li>
<li>To distill these extracted confidences back into the base model, finetune π-Base on the dataset of open-ended query and summary pairs {(q(i), s(i))}iN=1 to obtain the supervised finetuned (SFT) model.</li>
</ul>
<h3 id="decision-based-rl">Decision-Based RL</h3>
<p>Forecasting conditional on z is not a fundamentally challenging task. For example, if z provides a clear list of possible answers to the question x and associated percentage likelihoods, forecasting is a simple extractive task.</p>
<p><strong>RL objective</strong></p>
<p><img src="../imgs/band2024linguistic/image-4.png" alt="alt text"></p>
<h3 id="implementation">Implementation</h3>
<p>Decompose forecasting into two operations:</p>
<p><img src="../imgs/band2024linguistic/image-5.png" alt="alt text"></p>
<p><img src="../imgs/band2024linguistic/image-6.png" alt="alt text"></p>
<p><img src="../imgs/band2024linguistic/image-7.png" alt="alt text"></p>
<h2 id="experiments">Experiments</h2>
<h3 id="goals">Goals</h3>
<ol>
<li>LC provides better calibration with comparable accuracy.</li>
<li>LC is computationally tractable.</li>
<li>LC generalizes well out-of-distribution.</li>
</ol>
<h3 id="setup">Setup</h3>
<h4 id="models">Models</h4>
<ul>
<li>Llama 2 7B.</li>
</ul>
<h4 id="datasets">Datasets</h4>
<ul>
<li>TriviaQA.</li>
</ul>
<h4 id="metrics">Metrics</h4>
<ul>
<li>Expected Calibration Error (ECE).</li>
</ul>
<h4 id="baselines">Baselines</h4>
<ul>
<li>ICL.</li>
<li>Claude Distill.</li>
<li>Factuality SFT: Use the above ICL baseline to generate long-form responses over all queries in the SFT split, and finetune Llama 2 7B on these (query, response) pairs.</li>
<li>Factuality RL: Train a reward model which scores the correctness of long-form outputs and use it in RL.</li>
<li>Summary ICL.</li>
<li>GPT-4.</li>
</ul>
<p><img src="../imgs/band2024linguistic/image-8.png" alt="alt text"></p>
<h3 id="linguistic-calibration-using-question-answering-datasets">Linguistic Calibration using Question-Answering Datasets</h3>
<ul>
<li>Better ECE with comparable accuracy in long-form generation.</li>
<li>Reliability diagrams demonstrate meaningful confidences: LC model confidences are indeed both meaningful (they cover a wide range of confidence values), and are consistently close to the identity across confidence values.</li>
</ul>
<p><img src="../imgs/band2024linguistic/image-9.png" alt="alt text"></p>
<h3 id="zero-shot-generalization-to-a-biography-generation-task">Zero-Shot Generalization to a Biography Generation Task</h3>
<h2 id="future-work">Future Work</h2>
<ul>
<li>Investigating how LM interpretations of ambiguous linguistic confidence statements match up with human interpretations is important future work.</li>
<li>Future work should consider curating a more representative dataset of decision-making tasks, to improve LC’s generalization to user decisions in-the-wild.</li>
</ul>

    </body>
</html>